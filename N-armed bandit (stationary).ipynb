{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action value function Q_t(a): Incremental approach\n",
    "##Q_t+1(a) = Q_t(a) + (1/N_t(a))*(R_t(a) - Q_t(a))\n",
    "def sample(X,t,A,a,Q,step):\n",
    "    ind = (A[t] == a)\n",
    "    if ind == 1:\n",
    "        Q = Q + (1/step[a])*(X[t,a]-Q)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling from the softmax distribution function - for Gradient Bandit\n",
    "def softmax(H):   #H(a) = preference\n",
    "    n = len(H)\n",
    "    p = np.zeros((n))\n",
    "    for a in range(n):\n",
    "        p[a] = np.exp(H[a])/np.sum(np.exp(H))\n",
    "    p = list(p)\n",
    "    select = np.asscalar(np.random.choice(a=n,size=1,p=p))\n",
    "\n",
    "    return select, p #returns action, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Greedy method\n",
    "def Greedy():\n",
    "    s = n*2 #randomly start first s actions\n",
    "    A = np.zeros((t)) \n",
    "    step = np.zeros((n))\n",
    "    \n",
    "    reward_avg1 = np.zeros((m,t))\n",
    "    reward_avg2 = np.zeros((t))\n",
    "    \n",
    "    for k in range(m):\n",
    "        reward_tot = 0\n",
    "        \n",
    "        #initial random selections\n",
    "        for i in range(s): \n",
    "            A[i] = random.randint(0,n-1)\n",
    "            \n",
    "        for a in range(n):\n",
    "            step[a] =  sum(A[:s] == a)\n",
    "            \n",
    "        #iterate through the rest of steps\n",
    "        Q = X[k,s,:] \n",
    "        for i in range(s,t):\n",
    "            for a in range(n):\n",
    "                Q[a] =  sample(X[k,:,:],i-1,A,a,Q[a],step)\n",
    "            A[i] = np.argmax(Q) #greedy\n",
    "            \n",
    "            A = A.astype(int)\n",
    "            step[A[i]] += 1\n",
    "            reward_tot += X[k,i,A[i]]\n",
    "            reward_avg1[k,i] =  reward_tot/(i+1)\n",
    "     \n",
    "    #Compute average reward over all bandits\n",
    "    for i in range(t):\n",
    "        reward_avg2[i] = np.mean(reward_avg1[:,i])\n",
    "    \n",
    "    return reward_avg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Eps-Greedy (fixed eps)\n",
    "def EpsGreedy(eps):\n",
    "    s = n*2 #randomly start first s actions\n",
    "    A = np.zeros((t)) \n",
    "    step = np.zeros((n))\n",
    "    \n",
    "    reward_avg1 = np.zeros((m,t))\n",
    "    reward_avg2 = np.zeros((t))\n",
    "    \n",
    "    for k in range(m):\n",
    "        reward_tot = 0\n",
    "        \n",
    "        #initial random selections\n",
    "        for i in range(s): \n",
    "            A[i] = random.randint(0,n-1)\n",
    "            \n",
    "        for a in range(n):\n",
    "            step[a] =  sum(A[:s] == a)\n",
    "            \n",
    "        #iterate through the rest of steps\n",
    "        Q = X[k,s,:] \n",
    "        for i in range(s,t):\n",
    "            for a in range(n):\n",
    "                Q[a] =  sample(X[k,:,:],i-1,A,a,Q[a],step)\n",
    "            \n",
    "            #with prob eps, randomly explore\n",
    "            u = np.asscalar(np.random.uniform(0,1,1))\n",
    "            if u < eps:\n",
    "                A[i] = random.randint(0,n-1)\n",
    "            else:\n",
    "                A[i] = np.argmax(Q) #eps-greedy\n",
    "            \n",
    "            A = A.astype(int)\n",
    "            step[A[i]] += 1\n",
    "            reward_tot += X[k,i,A[i]]\n",
    "            reward_avg1[k,i] =  reward_tot/(i+1)\n",
    "     \n",
    "    #Compute average reward over all bandits\n",
    "    for i in range(t):\n",
    "        reward_avg2[i] = np.mean(reward_avg1[:,i])\n",
    "\n",
    "    return reward_avg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Eps-greedy (adaptive eps)\n",
    "def EpsGreedy2(EPS):\n",
    "    s = n*2 #randomly start first s actions\n",
    "    A = np.zeros((t)) \n",
    "    step = np.zeros((n))\n",
    "    \n",
    "    reward_avg1 = np.zeros((m,t))\n",
    "    reward_avg2 = np.zeros((t))\n",
    "    \n",
    "    for k in range(m):\n",
    "        reward_tot = 0\n",
    "        \n",
    "        #initial random selections\n",
    "        for i in range(s): \n",
    "            A[i] = random.randint(0,n-1)\n",
    "            \n",
    "        for a in range(n):\n",
    "            step[a] =  sum(A[:s] == a)\n",
    "            \n",
    "        #iterate through the rest of steps\n",
    "        Q = X[k,s,:] \n",
    "        for i in range(s,t):\n",
    "            for a in range(n):\n",
    "                Q[a] =  sample(X[k,:,:],i-1,A,a,Q[a],step)\n",
    "            \n",
    "            #with prob eps, randomly explore\n",
    "            u = np.asscalar(np.random.uniform(0,1,1))\n",
    "            if u < EPS[i]:\n",
    "                A[i] = random.randint(0,n-1)\n",
    "            else:\n",
    "                A[i] = np.argmax(Q) #eps-greedy\n",
    "            \n",
    "            A = A.astype(int)\n",
    "            step[A[i]] += 1\n",
    "            reward_tot += X[k,i,A[i]]\n",
    "            reward_avg1[k,i] =  reward_tot/(i+1)\n",
    "     \n",
    "    #Compute average reward over all bandits\n",
    "    for i in range(t):\n",
    "        reward_avg2[i] = np.mean(reward_avg1[:,i])\n",
    "\n",
    "    return reward_avg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Upper-Confidence-Bound (UCB)\n",
    "def UCB(c):\n",
    "    s = n*2 #randomly start first s actions\n",
    "    A = np.zeros((t)) \n",
    "    step = np.zeros((n))\n",
    "    \n",
    "    reward_avg1 = np.zeros((m,t))\n",
    "    reward_avg2 = np.zeros((t))\n",
    "    \n",
    "    for k in range(m):\n",
    "        reward_tot = 0\n",
    "        \n",
    "        #initial random selections\n",
    "        for i in range(s): \n",
    "            A[i] = random.randint(0,n-1)\n",
    "            \n",
    "        for a in range(n):\n",
    "            step[a] =  sum(A[:s] == a)\n",
    "            \n",
    "        #iterate through the rest of steps\n",
    "        Q = X[k,s,:] \n",
    "        UCB = np.zeros((n))\n",
    "        for i in range(s,t):\n",
    "            for a in range(n):\n",
    "                Q[a] =  sample(X[k,:,:],i-1,A,a,Q[a],step)\n",
    "                if step[a] == 0:\n",
    "                    UCB[a] = Q[a]\n",
    "                else:\n",
    "                    UCB[a] = Q[a] + c*np.sqrt(np.log(i)/step[a])\n",
    "            A[i] = np.argmax(UCB)\n",
    "            \n",
    "            A = A.astype(int)\n",
    "            step[A[i]] += 1\n",
    "            reward_tot += X[k,i,A[i]]\n",
    "            reward_avg1[k,i] =  reward_tot/(i+1)\n",
    "     \n",
    "    #Compute average reward over all bandits\n",
    "    for i in range(t):\n",
    "        reward_avg2[i] = np.mean(reward_avg1[:,i])\n",
    "\n",
    "    return reward_avg2         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Gradient Bandit\n",
    "def GB(alp):\n",
    "    step = np.zeros((n))    \n",
    "    reward_avg1 = np.zeros((m,t))\n",
    "    reward_avg2 = np.zeros((t))\n",
    "    \n",
    "    for k in range(m):\n",
    "        reward_tot = 0\n",
    "        A = np.zeros((t)) \n",
    "        \n",
    "        #First step of GB\n",
    "        H = X[k,0,:]\n",
    "        A0 = softmax(H)[0]\n",
    "        A[0] = A0\n",
    "        A = A.astype(int)\n",
    "        pi = softmax(H)[1]        \n",
    "        for a in range(n):\n",
    "            if a == A0:\n",
    "                H[a] = H[a] + alp*(1-pi[a])*X[k,0,a]\n",
    "            else:\n",
    "                H[a] = H[a] - alp*pi[a]*X[k,0,a]       \n",
    "        for a in range(n):\n",
    "            step[a] = (A[0] == a)        \n",
    "        reward_tot += X[k,0,A[0]]\n",
    "        reward_avg1[k,0] =  reward_tot\n",
    "        \n",
    "        #After first step of GB\n",
    "        Q = X[k,1,:]\n",
    "        for i in range(1,t):\n",
    "            pi = softmax(H)[1]\n",
    "            Ai = softmax(H)[0]\n",
    "            A[i] = Ai\n",
    "            A = A.astype(int)\n",
    "            reward_tot += X[k,i,A[i]]\n",
    "            reward_avg1[k,i] =  reward_tot/(i+1)\n",
    "            step[A[i]] += 1\n",
    "            \n",
    "            for a in range(n):\n",
    "                Q[a] = sample(X[k,:,:],i-1,A,a,Q[a],step)\n",
    "                if a == A[i]:\n",
    "                    H[a] = H[a] + alp*(1-pi[a])*(X[k,i,a]-Q[a])\n",
    "                else:\n",
    "                    H[a] = H[a] - alp*pi[a]*(X[k,i,a]-Q[a]) \n",
    "             \n",
    "    #Compute average reward over all bandits\n",
    "    for i in range(t):\n",
    "        reward_avg2[i] = np.mean(reward_avg1[:,i])\n",
    "\n",
    "    return reward_avg2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Gradient Bandit - Adaptive Step Size\n",
    "def GB2(ALP):\n",
    "    step = np.zeros((n))    \n",
    "    reward_avg1 = np.zeros((m,t))\n",
    "    reward_avg2 = np.zeros((t))\n",
    "    \n",
    "    for k in range(m):\n",
    "        reward_tot = 0\n",
    "        A = np.zeros((t)) \n",
    "        \n",
    "        #First step of GB\n",
    "        H = X[k,0,:]\n",
    "        A0 = softmax(H)[0]\n",
    "        A[0] = A0\n",
    "        A = A.astype(int)\n",
    "        pi = softmax(H)[1]        \n",
    "        for a in range(n):\n",
    "            if a == A0:\n",
    "                H[a] = H[a] + ALP[0]*(1-pi[a])*X[k,0,a]\n",
    "            else:\n",
    "                H[a] = H[a] - ALP[0]*pi[a]*X[k,0,a]       \n",
    "        for a in range(n):\n",
    "            step[a] = (A[0] == a)        \n",
    "        reward_tot += X[k,0,A[0]]\n",
    "        reward_avg1[k,0] =  reward_tot\n",
    "        \n",
    "        #After first step of GB\n",
    "        Q = X[k,1,:]\n",
    "        for i in range(1,t):\n",
    "            pi = softmax(H)[1]\n",
    "            Ai = softmax(H)[0]\n",
    "            A[i] = Ai\n",
    "            A = A.astype(int)\n",
    "            reward_tot += X[k,i,A[i]]\n",
    "            reward_avg1[k,i] =  reward_tot/(i+1)\n",
    "            step[A[i]] += 1\n",
    "            \n",
    "            for a in range(n):\n",
    "                Q[a] = sample(X[k,:,:],i-1,A,a,Q[a],step)\n",
    "                if a == A[i]:\n",
    "                    H[a] = H[a] + ALP[i]*(1-pi[a])*(X[k,i,a]-Q[a])\n",
    "                else:\n",
    "                    H[a] = H[a] - ALP[i]*pi[a]*(X[k,i,a]-Q[a]) \n",
    "             \n",
    "    #Compute average reward over all bandits\n",
    "    for i in range(t):\n",
    "        reward_avg2[i] = np.mean(reward_avg1[:,i])\n",
    "\n",
    "    return reward_avg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameters\n",
    "m,n,t,eps1,eps2,c,alp = 5,10,1000,0.1,0.01,1.96,0.001\n",
    "EPS = np.sort(np.arange(5,5+t,1)/10000)[::-1]\n",
    "ALP = np.sort(np.arange(1,5+t,1)/10000)[::-1]\n",
    "step = np.arange(0,t,1)[n*2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scenario 1.  Noisy continous reward\n",
    "X = np.empty((m,t,n)) #Reward matrix\n",
    "mu1,sigma1 = 0,1\n",
    "mu2,sigma2 = 0,2\n",
    "\n",
    "for k in range(m): #averaged over m bandits\n",
    "    qa = np.random.normal(mu1,sigma1,n) #True action values\n",
    "    for j in range(t): #1000 time steps\n",
    "        X[k,j,:] = qa + np.random.normal(mu2,sigma2,n) #Noisy reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = Greedy()\n",
    "result2 = EpsGreedy(eps1)\n",
    "result3 = EpsGreedy(eps2)\n",
    "result4 = EpsGreedy2(EPS)\n",
    "result5 = UCB(c)\n",
    "result6 = GB(alp)\n",
    "result7 = GB2(ALP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average reward plot\n",
    "fig = plt.figure()\n",
    "plt.plot(step,result1[n*2:],'r--',label='Greedy')\n",
    "plt.plot(step,result2[n*2:],'y--',label='Eps = 0.1')\n",
    "plt.plot(step,result3[n*2:],'g--',label='Eps = 0.01')\n",
    "plt.plot(step,result4[n*2:],'b--',label='Adaptive Eps')\n",
    "plt.plot(step,result5[n*2:],'m--',label='UCB')\n",
    "plt.plot(step,result6[n*2:],'c--',label='Gradient (fixed)')\n",
    "plt.plot(step,result7[n*2:],'k--',label='Gradient (adaptive)')\n",
    "plt.title('Average Reward Plot', size=16, fontweight='bold')\n",
    "plt.xlabel('Time Steps', fontsize=18)\n",
    "plt.ylabel('Average Reward', fontsize=18)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set parameters\n",
    "m,n,t,eps1,eps2,c,alp = 5,4,1000,0.1,0.01,1,0.05\n",
    "EPS = np.sort(np.arange(5,5+t,1)/10000)[::-1]\n",
    "ALP = np.sort(np.arange(5,5+t,1)/10000)[::-1]\n",
    "step = np.arange(0,t,1)[n*2:]\n",
    "\n",
    "#Scenario 2.  Bernoulli Bandit: each reward can be either success or failure\n",
    "X = np.zeros((m,t,n)) #Reward matrix\n",
    "\n",
    "for k in range(m): #averaged over m bandits\n",
    "    p = [0.5,0.7,0.4,0.3]\n",
    "    for j in range(t): #1000 time steps\n",
    "        X[k,j,:] = np.random.binomial(n=1, p = p, size = (1,n)) #Noisy reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = Greedy()\n",
    "result2 = EpsGreedy(eps1)\n",
    "result3 = EpsGreedy(eps2)\n",
    "result4 = EpsGreedy2(EPS)\n",
    "result5 = UCB(c)\n",
    "result6 = GB(alp)\n",
    "result7 = GB2(ALP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average reward plot\n",
    "fig = plt.figure()\n",
    "plt.plot(step,result1[n*2:],'r--',label='Greedy')\n",
    "plt.plot(step,result2[n*2:],'y--',label='Eps = 0.1')\n",
    "plt.plot(step,result3[n*2:],'g--',label='Eps = 0.01')\n",
    "plt.plot(step,result4[n*2:],'b--',label='Adaptive Eps')\n",
    "plt.plot(step,result5[n*2:],'m--',label='UCB')\n",
    "plt.plot(step,result6[n*2:],'c--',label='Gradient (fixed)')\n",
    "plt.plot(step,result7[n*2:],'k--',label='Gradient (adaptive)')\n",
    "plt.title('Average Reward Plot', size=16, fontweight='bold')\n",
    "plt.xlabel('Time Steps', fontsize=18)\n",
    "plt.ylabel('Average Reward', fontsize=18)\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
